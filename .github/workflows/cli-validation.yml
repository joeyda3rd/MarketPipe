name: CLI Validation Framework

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run daily at 6 AM UTC to catch any regressions
    - cron: '0 6 * * *'

jobs:
  cli-command-matrix:
    name: CLI Command Matrix Validation
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.8', '3.9', '3.10', '3.11']
      fail-fast: false
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest pytest-xdist pytest-cov
    
    - name: Run CLI Command Matrix Tests
      run: |
        python -m pytest tests/integration/test_cli_command_matrix.py -v \
          --tb=short \
          --junitxml=reports/cli-matrix-${{ matrix.python-version }}.xml
    
    - name: Upload CLI Matrix Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: cli-matrix-results-${{ matrix.python-version }}
        path: reports/cli-matrix-${{ matrix.python-version }}.xml

  cli-option-validation:
    name: CLI Option Validation
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.11']  # Test on subset for speed
      fail-fast: false
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest pytest-xdist pytest-cov
    
    - name: Run CLI Option Validation Tests
      run: |
        python -m pytest tests/integration/test_cli_option_validation.py -v \
          --tb=short \
          --junitxml=reports/cli-options-${{ matrix.python-version }}.xml
    
    - name: Upload CLI Option Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: cli-option-results-${{ matrix.python-version }}
        path: reports/cli-options-${{ matrix.python-version }}.xml

  pipeline-smoke-tests:
    name: Pipeline Smoke Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.11']
      fail-fast: false
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest pytest-xdist pytest-cov pandas
    
    - name: Run Pipeline Smoke Tests
      run: |
        python -m pytest tests/integration/test_pipeline_smoke_validation.py -v \
          --tb=short \
          -m "not slow" \
          --junitxml=reports/pipeline-smoke-${{ matrix.python-version }}.xml
    
    - name: Upload Pipeline Smoke Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: pipeline-smoke-results-${{ matrix.python-version }}
        path: reports/pipeline-smoke-${{ matrix.python-version }}.xml

  provider-validation:
    name: Provider Validation
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.11']
      fail-fast: false
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest pytest-xdist pytest-cov
    
    - name: Run Provider Validation Tests
      run: |
        python -m pytest tests/integration/test_provider_specific_validation.py -v \
          --tb=short \
          -m "not auth_required" \
          --junitxml=reports/provider-validation-${{ matrix.python-version }}.xml
    
    - name: Upload Provider Validation Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: provider-validation-results-${{ matrix.python-version }}
        path: reports/provider-validation-${{ matrix.python-version }}.xml

  backward-compatibility:
    name: Backward Compatibility Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.11']
      fail-fast: false
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest pytest-xdist pytest-cov
    
    - name: Run Backward Compatibility Tests
      run: |
        python -m pytest tests/integration/test_cli_backward_compatibility.py -v \
          --tb=short \
          --junitxml=reports/backward-compatibility-${{ matrix.python-version }}.xml
    
    - name: Upload Backward Compatibility Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: backward-compatibility-results-${{ matrix.python-version }}
        path: reports/backward-compatibility-${{ matrix.python-version }}.xml

  config-schema-validation:
    name: Configuration Schema Validation
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.11']
      fail-fast: false
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest pytest-xdist pytest-cov
    
    - name: Run Configuration Schema Tests
      run: |
        python -m pytest tests/integration/test_config_schema_validation.py -v \
          --tb=short \
          --junitxml=reports/config-schema-${{ matrix.python-version }}.xml
    
    - name: Upload Configuration Schema Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: config-schema-results-${{ matrix.python-version }}
        path: reports/config-schema-${{ matrix.python-version }}.xml

  health-check-validation:
    name: Health Check Validation
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.11']
      fail-fast: false
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest pytest-xdist pytest-cov
    
    - name: Test Health Check Command
      run: |
        python -m marketpipe health-check --verbose
    
    - name: Test Health Check Help
      run: |
        python -m marketpipe health-check --help
    
    - name: Test Health Check with Config
      run: |
        echo "ingestion:" > test_config.yaml
        echo "  symbols: ['AAPL']" >> test_config.yaml
        echo "  start_date: '2023-01-01'" >> test_config.yaml
        echo "  end_date: '2023-01-01'" >> test_config.yaml
        python -m marketpipe health-check --config test_config.yaml --verbose

  performance-baseline:
    name: Performance Baseline Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.11']  # Only test on latest for performance
      fail-fast: false
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest pytest-xdist pytest-cov pandas
    
    - name: Run Performance Baseline Tests
      run: |
        python -m pytest tests/integration/test_pipeline_smoke_validation.py::TestPipelineSmokeValidation::test_performance_baseline -v \
          --tb=short \
          --junitxml=reports/performance-baseline-${{ matrix.python-version }}.xml
    
    - name: Upload Performance Baseline Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-baseline-results-${{ matrix.python-version }}
        path: reports/performance-baseline-${{ matrix.python-version }}.xml

  cli-validation-report:
    name: Generate CLI Validation Report
    runs-on: ubuntu-latest
    needs: [
      cli-command-matrix,
      cli-option-validation, 
      pipeline-smoke-tests,
      provider-validation,
      backward-compatibility,
      config-schema-validation,
      health-check-validation,
      performance-baseline
    ]
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
    
    - name: Download all artifacts
      uses: actions/download-artifact@v3
      with:
        path: artifacts/
    
    - name: Generate Comprehensive CLI Validation Report
      run: |
        python << 'EOF'
        import os
        import xml.etree.ElementTree as ET
        from pathlib import Path
        from datetime import datetime
        
        def parse_junit_xml(file_path):
            """Parse JUnit XML file and extract test results."""
            try:
                tree = ET.parse(file_path)
                root = tree.getroot()
                
                tests = int(root.get('tests', 0))
                failures = int(root.get('failures', 0))
                errors = int(root.get('errors', 0))
                time = float(root.get('time', 0))
                
                return {
                    'tests': tests,
                    'failures': failures,
                    'errors': errors,
                    'passed': tests - failures - errors,
                    'time': time,
                    'success_rate': (tests - failures - errors) / tests * 100 if tests > 0 else 0
                }
            except Exception as e:
                return {'error': str(e)}
        
        # Find all XML files
        artifacts_dir = Path('artifacts')
        xml_files = list(artifacts_dir.rglob('*.xml'))
        
        # Generate report
        report = []
        report.append("# MarketPipe CLI Validation Framework Report")
        report.append("=" * 60)
        report.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}")
        report.append("")
        
        # Summary by test suite
        test_suites = {}
        total_tests = 0
        total_passed = 0
        total_failures = 0
        total_errors = 0
        total_time = 0
        
        for xml_file in xml_files:
            suite_name = xml_file.parent.name.replace('-results-', ' ').replace('-', ' ').title()
            python_version = xml_file.stem.split('-')[-1]
            
            results = parse_junit_xml(xml_file)
            if 'error' not in results:
                if suite_name not in test_suites:
                    test_suites[suite_name] = {}
                test_suites[suite_name][python_version] = results
                
                total_tests += results['tests']
                total_passed += results['passed']
                total_failures += results['failures']
                total_errors += results['errors']
                total_time += results['time']
        
        # Overall summary
        overall_success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
        report.append("## Overall Summary")
        report.append(f"- **Total Tests**: {total_tests}")
        report.append(f"- **Passed**: {total_passed}")
        report.append(f"- **Failed**: {total_failures}")
        report.append(f"- **Errors**: {total_errors}")
        report.append(f"- **Success Rate**: {overall_success_rate:.1f}%")
        report.append(f"- **Total Execution Time**: {total_time:.2f}s")
        report.append("")
        
        # Suite breakdown
        report.append("## Test Suite Results")
        report.append("")
        
        for suite_name, versions in test_suites.items():
            report.append(f"### {suite_name}")
            
            for version, results in versions.items():
                status = "✅" if results['failures'] == 0 and results['errors'] == 0 else "❌"
                report.append(f"- **Python {version}**: {status} "
                            f"{results['passed']}/{results['tests']} passed "
                            f"({results['success_rate']:.1f}%) "
                            f"in {results['time']:.2f}s")
            
            report.append("")
        
        # Status indicators
        report.append("## Status Indicators")
        if total_failures == 0 and total_errors == 0:
            report.append("🟢 **ALL TESTS PASSING** - CLI validation framework is healthy")
        elif total_failures > 0 or total_errors > 0:
            report.append("🔴 **TESTS FAILING** - CLI validation framework detected issues")
            
            if total_failures > 0:
                report.append(f"   - {total_failures} test failures detected")
            if total_errors > 0:
                report.append(f"   - {total_errors} test errors detected")
        
        report.append("")
        
        # Recommendations
        if total_failures > 0 or total_errors > 0:
            report.append("## Recommendations")
            report.append("1. Review failing tests in the individual job logs")
            report.append("2. Run health check command: `marketpipe health-check --verbose`")
            report.append("3. Verify that CLI changes maintain backward compatibility")
            report.append("4. Test configuration changes with validation framework")
            report.append("")
        
        # Save report
        with open('CLI_VALIDATION_REPORT.md', 'w') as f:
            f.write('\n'.join(report))
        
        print('\n'.join(report))
        EOF
    
    - name: Upload CLI Validation Report
      uses: actions/upload-artifact@v3
      with:
        name: cli-validation-report
        path: CLI_VALIDATION_REPORT.md
    
    - name: Comment PR with Results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const report = fs.readFileSync('CLI_VALIDATION_REPORT.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: `## CLI Validation Framework Results\n\n${report}`
          });

  # Weekly comprehensive validation (includes auth-required tests)
  weekly-comprehensive:
    name: Weekly Comprehensive Validation
    runs-on: ubuntu-latest
    if: github.event.schedule == '0 6 * * 0'  # Only run on weekly schedule
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest pytest-xdist pytest-cov pandas
    
    # Note: This would require secrets for API keys
    # - name: Run Auth-Required Provider Tests
    #   env:
    #     ALPACA_KEY: ${{ secrets.ALPACA_KEY }}
    #     ALPACA_SECRET: ${{ secrets.ALPACA_SECRET }}
    #     IEX_TOKEN: ${{ secrets.IEX_TOKEN }}
    #   run: |
    #     python -m pytest tests/integration/test_provider_specific_validation.py -v \
    #       -m "auth_required" \
    #       --tb=short
    
    - name: Run Performance Tests
      run: |
        python -m pytest tests/integration/test_pipeline_smoke_validation.py -v \
          -m "slow" \
          --tb=short
    
    - name: Generate Weekly Report
      run: |
        echo "Weekly comprehensive CLI validation completed"
        python -m marketpipe health-check --verbose --output weekly_health_report.txt
    
    - name: Upload Weekly Report
      uses: actions/upload-artifact@v3
      with:
        name: weekly-comprehensive-report
        path: weekly_health_report.txt